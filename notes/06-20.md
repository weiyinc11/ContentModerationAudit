# Research Meeting #2 - 06/20
Arjun + Geralyn : Weekly meetings established: Wed, 2 - 3:30pm
Danish Meeting: Thursday, 11am

## Review of Phases
### 1. Exploration of Platforms
   - Report on the tools that these platforms provide
     - Identifying adjacent papers that experiment on platforms : Mindmap of Related Papers
     - API details and mechanics behind what can be customized
     - Utilize the white papers and transparency reports to gain a deeper insight into how these platforms are implementing autoMods
     - Highlight the keywords and policies within community guidelines of each of the platforms that seem impractical as well as most common policies that are demonstrated among these platforms explored.
       - Enforcement of these policies : how do they enforce these "We do not tolerate statements"
       - Other policies or rules around AI-generated content   
   - Platforms are not supplying an LLM for access for moderators - perhaps due to Latency
     - Explain that this is the current system that platforms can afford.. Many platforms may utilize GPT and such to implement autoMod
     - Or perhaps they are not able to implement these functionality to a certain confidence
### 2. Utilizing NLP / LLM systems
  - Experimental Design
    - Line rate, Automated input of data / content onto these platforms
    - Narrow down into which policies we would like to investigate and focus on
  - NLP datasets and testing against baseline systems
    - Datasets that are tailored towards focused policies
    - Perhaps if these policies are not well developed in the world of NLP, what are these platforms using to inform their LLMs for content moderation
### 3. Testing the robustness of the systems
  - Tweaking the data inputs and attempting to identify where the LLMs are most successful or may falter in performance images.
    - Most platforms could be utilizing a streamlined API such as Perspective where the process of breaking these systems would be less essential

