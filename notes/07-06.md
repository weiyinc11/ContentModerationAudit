# Research Meeting - 07/06

## Testing and Experimental Design
- Testing sophisticated and grey area content moderation needs.
- How do platforms deal with grey area stuff?

## Review of Twitch Tools
- AutoModerator functions: implementing rules on a creator level
- Machine Detection: use of LLMs without clear documentation about what they are using Machine Detection for
  > Identify whether the machine detection tool is a platform wide tool and whether or not we can test it on a creator level.

- Viewer Safety: to what extent do the tools moderate or block the content
- Shield Mode: Making your stream less accessible due to attacks on creators like Raid Attacks

## AutoMod walkthrough (Tools to test) 
- Smart Detection: beyond basic moderation rules, certain messages are moderated because of the human moderator community
- Suspicious user detection: removing users based on multiple occurences of policy and comm guideline violations

## Experimental Design
- Needs to distinguish between which tools (User or Content/Message level)

## Discord vs Twitch
- Discord has a mature extension community vs Twitch encourages more internal moderation tools.
- Categorize the tools under levels of ML use to support the implementation of them

## Moderation levels and categories
- Platform-level policies
- Community based guidelines within a stream or a server
- Keyword detection on a user to user content / message

> Community-specific rules that content moderation systems need to take into account and the need for personalizing the content moderated according to each communities' guidelines.
