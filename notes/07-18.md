# Agenda
- Geralyn has mapped out the Twitch moderation structure and what data we may need
- Yash has run the scraper, check in on the process and results
- Key decisions to be made:
    - Are we testing only language or images/video as well? (Both Twitch and Discord have tools for the latter)
    - What are the key metrics to test, i.e. how do we measure effectiveness of moderation?
        - Obv. some fraction, but fraction of what? 
        - Stratify the data by perceived ease of moderation?
- Decide on baseline set of experiments:
    - Twitch 
        - AutoMod (hate speech)
        - Chat filters
    - Discord
        - ?
- Future exploration:
    - Twitch
        - Smart detection (trained based on moderation actions)

# Notes
1. Review of the Experimental Design and ML-based tools that Twitch provides
 - Discussed levels of moderating and filtering through the messages/comments in a chat --> AutoMod provides these levels of no filtering, medium, and heavy. What are the implications of these levels or what is the meaning of a medium level of moderation?
 - Categories and Policies to shape our dataset of harmful comments
    - Sexual Content and Profanity
        - Both include content that is 1. not desired to be constantly at exposure towards, 2. simple and not include sophisticated instances of harmful content to be moderated
    - Interpersonal Hate speech and Hate speech seem to provide areas of ambiguity and room for investigation.
        - Aggression and Bullying
        - Hate Speech towards Race, Sexuality, Religion, and etc.
 - Mode of Content for Investigation: Language Based
 - Interesting tool that could be used for initial testing and working out the kinks of our exp design: Machine Detection based Links to Malware checks but need a dataset of malware links
     - Utilizes regex-based detection for malware links
 - Interestingly, no mention of misinformation?
 - *! Q -> How many messages do we need to get for smart detection to learn niche rules of different streams and communities?*
2. Methods of Retrieving datasets
! -> need to identify policies from scaper results and check with discord on their categories of content banned (Just in case of dual investigation)
 - For some publically available models, we can use some jailbreaks to retrieve content.
 - Depending on the content being contextualized within a conversation or based on singular standalone statements
 - Would be great to produce varying levels of harmfulness such that there are ambiguity to levels of hate speech
 - Often times in the real world, the harmful content provided not be cleanly cut into certain categories -- what happens when these categories mix?
 - ! Workflow: Collated list of policies (mostly regarding the categories of hate speech and bullying) -> Associated with either a real human dataset / ML generated -> Verify and Categorize based on human dataset
3. Logistics of Experimental Design
 - Phone Verification + Email for Multiple Bot testing
     - burner.com and other burner phone sites under investigation
 - Line Rate of Messages and a need for the buffer
     - Because there is a limit of 20 messages / 30 seconds, we need to buffer to avoid being locked out. We also need to consider the time min of a couple hours per policy in testing.
4. Discussion of Metrics of Investigation
 - Content Moderated / All Content Provided to the AutoMod (Under Max Filtering)
 - Also need to get the ratio of harmful content across categories (Can be found in transparency reports but detection bias)
     - If content is more or less prevalent, scale it accordingly in our dataset. Or if there is less prevalence, perhaps their tool is weaker
